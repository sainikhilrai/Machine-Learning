{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gradient descent method\n",
    "def gdm(learningR,toleranceV,theta,thetaOld,n_classes,input_size,lambda_,input_data,labels):\n",
    "    while (np.linalg.norm(theta - thetaOld,ord = 2) >= toleranceV):\n",
    "        thetaOld = theta\n",
    "        cost,derivative = softmax_cost(theta, n_classes, input_size, lambda_, input_data, labels)\n",
    "        theta = thetaOld - learningR * derivative\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_cost(theta, n_classes, input_size, lambda_, data, labels):\n",
    "    \"\"\"\n",
    "    Compute the cost and derivative.\n",
    "    n_classes: the number of classes\n",
    "    input_size: the size N of the input vector\n",
    "    lambda_: weight decay parameter\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    labels: an M x 1 matrix containing the labels corresponding for the input data\n",
    "    \"\"\"\n",
    "    # k stands for the number of classes\n",
    "    # n is the number of features and m is the number of samples\n",
    "    k = n_classes\n",
    "    n, m = data.shape\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Matrix of indicator fuction with shape (k, m)\n",
    "    labels = np.reshape(labels,(m,))\n",
    "    indicator = scipy.sparse.csr_matrix((np.ones(m), (labels, np.arange(m))))\n",
    "    indicator = np.array(indicator.todense())\n",
    "    # Cost function\n",
    "    cost = -1.0/m * np.sum(indicator * np.log(proba)) + 0.5*lambda_*np.sum(theta*theta)\n",
    "    # Gradient matrix with shape (k, n)\n",
    "    grad = -1.0/m * (indicator - proba).dot(data.T) + lambda_*theta\n",
    "    # Unroll the gradient matrices into a vector\n",
    "    grad = grad.ravel()\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_train(input_size, n_classes, lambda_, input_data, labels, options={'maxiter': 400, 'disp': True}):\n",
    "    # initialize parameters\n",
    "    theta = 0.005 * np.random.randn(n_classes * input_size)\n",
    "    thetaOld = theta * 1000\n",
    "\n",
    "    # Find out the optimal theta\n",
    "    opt_theta =  gdm(0.01,0.01,theta,thetaOld,n_classes,input_size,lambda_,input_data,labels)\n",
    "    opt_theta = np.array(opt_theta)\n",
    "    model = {'opt_theta': opt_theta, 'n_classes': n_classes, 'input_size': input_size}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_predict(model, data):\n",
    "    \"\"\"\n",
    "    model: model trained using softmax_train\n",
    "    data: the N x M input matrix, where each column data[:, i] corresponds to\n",
    "          a single test set\n",
    "    pred: the prediction array.\n",
    "    \"\"\"\n",
    "    theta = model['opt_theta'] # Optimal theta\n",
    "    k = model['n_classes']  # Number of classes\n",
    "    n = model['input_size'] # Input size (number of features)\n",
    "    # Reshape theta\n",
    "    theta = theta.reshape((k, n))\n",
    "    # Probability with shape (k, m)\n",
    "    theta_data = theta.dot(data)\n",
    "    alpha = np.max(theta_data, axis=0)\n",
    "    theta_data -= alpha # Avoid numerical problem due to large values of exp(theta_data)\n",
    "    proba = np.exp(theta_data) / np.sum(np.exp(theta_data), axis=0)\n",
    "    # Prediction values\n",
    "    pred = np.argmax(proba, axis=0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "X = loadmat('TrainImages.mat')\n",
    "X = X['trainData'].T\n",
    "y = loadmat('TrainLabels.mat')\n",
    "y = y['trainLabels']\n",
    "X_test = loadmat('TestImages.mat')\n",
    "X_test = X_test['testData'].T\n",
    "y_test = loadmat('TestLabels.mat')\n",
    "y_test = y_test['testLabels']\n",
    "print X.shape\n",
    "print X_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "input_size = 28 * 28 # Size of input vector (MNIST images are 28x28)\n",
    "n_classes = 10       # Number of classes (MNIST images fall into 10 classes)\n",
    "lambda_ = 1e-4 # Weight decay parameter\n",
    "print lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'maxiter': 100, 'disp': True}\n",
    "model = softmax_train(input_size, n_classes, lambda_, X, y, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6413\n"
     ]
    }
   ],
   "source": [
    "pred = softmax_predict(model, X_test)\n",
    "y_test = y_test.tolist()\n",
    "pred = pred.tolist()\n",
    "print accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this program, we used a gradient descent method for finding the optimum theta. We tried out various combination of the tolerance values, each of which gave us different accuracy. It was observed that as we go on decreasing the tolerance value, our accuracy was increasing. The following table summarizes the accuracy that we got when we went on decreasing the tolerance value. It was also observed that gradient descent method was taking more time to find the optimum theata for small values of tolerance values.\n",
    "\n",
    "|learning rate|tolerance Value|Accuracy|\n",
    "|:---:|:---:|:---:|\n",
    "|0.01|0.01|64.13|\n",
    "|0.01|0.09|75.37|\n",
    "|0.01|0.006|80.46|\n",
    "|0.01|0.003|84.43|\n",
    "|0.01|0.001|88.00|\n",
    "|0.01|0,0009|88.14|\n",
    "|0.01|0.0006|89.23|\n",
    "|0.01|0.0003|90.51|\n",
    "\n",
    "The following table compares the accuracy of model using gradient descent method and L-BFGS-B optimising method:\n",
    "\n",
    "|Model|Accuracy|\n",
    "|:---:|:---:|\n",
    "|GDM| |90.17|\n",
    "|L-BFGS| |92.58|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
